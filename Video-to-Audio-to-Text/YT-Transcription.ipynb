{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: brew\n"
     ]
    }
   ],
   "source": [
    "!brew install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/ffmpeg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Add the directory containing ffmpeg to the PATH in the notebook\n",
    "os.environ['PATH'] += os.pathsep + '/usr/local/bin'\n",
    "\n",
    "# Check if ffmpeg is now recognized\n",
    "!which ffmpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/ffprobe\n"
     ]
    }
   ],
   "source": [
    "!which ffprobe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import environ\n",
    "import yt_dlp\n",
    "import openai\n",
    "from pydub import AudioSegment\n",
    "from pyannote.audio import Pipeline\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Set your OpenAI API key\n",
    "client.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Hugging Face API key\n",
    "HF_API_Key = os.environ[\"HF_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    " # Specify the location of ffmpeg\n",
    "        'ffmpeg_location': '/usr/local/bin/ffmpeg',  # Use the path found via `which ffmpeg`\n",
    "        'outtmpl': output_file\n",
    "```        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "urls = [\n",
    "        \"https://www.youtube.com/\",\n",
    "        \"https://www.youtube.com/\",\n",
    "        \"https://www.youtube.com/\"\n",
    "    ]\n",
    "\n",
    "def download_and_convert(url):\n",
    "    try:\n",
    "        ydl_opts = {\n",
    "            'format': 'bestaudio/best',\n",
    "            'postprocessors': [{\n",
    "                'key': 'FFmpegExtractAudio',\n",
    "                'preferredcodec': 'wav',\n",
    "            }],\n",
    "            'outtmpl': '%(title)s.%(ext)s',\n",
    "        }\n",
    "\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            info = ydl.extract_info(url, download=True)\n",
    "            filename = ydl.prepare_filename(info).replace('.webm', '.wav')\n",
    "\n",
    "        # Convert to 16kHz mono\n",
    "        output_filename = f\"{os.path.splitext(filename)[0]}_16khz_mono.wav\"\n",
    "        cmd = [\n",
    "            \"ffmpeg\",\n",
    "            \"-i\", filename,\n",
    "            \"-ar\", \"16000\",\n",
    "            \"-ac\", \"1\",\n",
    "            \"-c:a\", \"pcm_s16le\",\n",
    "            output_filename\n",
    "        ]\n",
    "        subprocess.run(cmd, check=True)\n",
    "\n",
    "        # Clean up original WAV file\n",
    "        os.remove(filename)\n",
    "\n",
    "        print(f\"Processed: {output_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {str(e)}\")\n",
    "\n",
    "for url in urls:\n",
    "    download_and_convert(url)\n",
    "\n",
    "print(\"All videos processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai pyannote.audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import environ\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Set your OpenAI API key\n",
    "client.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Set your Hugging Face API key\n",
    "HF_API_Key = os.environ[\"HF_API_KEY\"]\n",
    "\n",
    "# Initialize the speaker diarization pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\")\n",
    "\n",
    "# Path to the directory containing the .wav files\n",
    "wav_directory = \"~/wd/GitHub/Research/Ops-Research/audio_files\"  # Adjust this path as needed\n",
    "\n",
    "def transcribe_audio_with_speaker_diarization(audio_file_path):\n",
    "    # Perform transcription using OpenAI's Whisper\n",
    "    with open(audio_file_path, 'rb') as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file)\n",
    "    print(transcript.text)\n",
    "    \n",
    "    # Perform speaker diarization using pyannote.audio\n",
    "    diarization = pipeline({'uri': audio_file_path, 'audio': audio_file_path})\n",
    "\n",
    "    # Combine transcription with speaker labels\n",
    "    transcribed_text = transcript['text']\n",
    "    speaker_segments = []\n",
    "\n",
    "    for segment, track, label in diarization.itertracks(yield_label=True):\n",
    "        speaker_segments.append((segment.start, segment.end, label))\n",
    "    \n",
    "    labeled_transcription = []\n",
    "\n",
    "    for start, end, speaker in speaker_segments:\n",
    "        # Match the speaker segments with the transcription (approximate)\n",
    "        labeled_transcription.append(f\"Speaker {speaker}: {transcribed_text}\")\n",
    "    \n",
    "    return \"\\n\".join(labeled_transcription)\n",
    "\n",
    "def process_wav_files(directory):\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\"_16khz_mono.wav\"):\n",
    "            audio_file_path = os.path.join(directory, file_name)\n",
    "            transcription_file = os.path.splitext(audio_file_path)[0] + \"_transcription.txt\"\n",
    "            \n",
    "            transcription_text = transcribe_audio_with_speaker_diarization(audio_file_path)\n",
    "            \n",
    "            with open(transcription_file, 'w') as f:\n",
    "                f.write(transcription_text)\n",
    "            \n",
    "            print(f\"Transcription saved: {transcription_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_wav_files(wav_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from pyannote.audio import Pipeline\n",
    "from openai import OpenAI\n",
    "from os import environ\n",
    "\n",
    "# Set your OpenAI API key\n",
    "client.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Set your Hugging Face API key (if needed for pyannote)\n",
    "HF_API_Key = os.environ[\"HF_API_KEY\"]\n",
    "\n",
    "# Initialize the speaker diarization pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\", use_auth_token=HF_API_Key)\n",
    "\n",
    "# Path to the directory containing the .wav files\n",
    "wav_directory = \"~/wd/GitHub/Research/Ops-Research/\"  # Adjust this path as needed\n",
    "\n",
    "audio_file_path = \"~/wd/GitHub/Research/Ops-Research/audio_files/liron_audio_1.wav\"\n",
    "#transcription_file = \"~/wd/GitHub/Research/Ops-Research/transcription_files/liron_audio_1_transcription.txt\"\n",
    "\n",
    "def transcribe_audio_with_speaker_diarization(audio_file_path):\n",
    "    # Perform transcription using OpenAI's Whisper\n",
    "    with open(audio_file_path, 'rb') as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file)\n",
    "    print(transcript.text)\n",
    "    # Perform speaker diarization using pyannote.audio\n",
    "    diarization = pipeline({'uri': audio_file_path, 'audio': audio_file_path})\n",
    "\n",
    "    # Combine transcription with speaker labels\n",
    "    transcribed_text = transcript['text']\n",
    "    speaker_segments = []\n",
    "\n",
    "    for segment, track, label in diarization.itertracks(yield_label=True):\n",
    "        speaker_segments.append((segment.start, segment.end, label))\n",
    "    \n",
    "    labeled_transcription = []\n",
    "\n",
    "    for start, end, speaker in speaker_segments:\n",
    "        # Match the speaker segments with the transcription (approximate)\n",
    "        labeled_transcription.append(f\"Speaker {speaker}: {transcribed_text}\")\n",
    "    \n",
    "    return \"\\n\".join(labeled_transcription)\n",
    "\n",
    "def process_wav_files(directory):\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".wav\"):\n",
    "            audio_file_path = os.path.join(directory, file_name)\n",
    "            transcription_file = os.path.splitext(audio_file_path)[0] + \"_transcription.txt\"\n",
    "            \n",
    "            transcription_text = transcribe_audio_with_speaker_diarization(audio_file_path)\n",
    "            \n",
    "            with open(transcription_file, 'w') as f:\n",
    "                f.write(transcription_text)\n",
    "            \n",
    "            print(f\"Transcription saved: {transcription_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_wav_files(wav_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe the .wav file to a .srt file with speaker labels\n",
    "import os\n",
    "import openai\n",
    "from pyannote.audio import Pipeline\n",
    "from openai import OpenAI\n",
    "from os import environ\n",
    "\n",
    "# Set your OpenAI API key\n",
    "client.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Set your Hugging Face API key (if needed for pyannote)\n",
    "HF_API_Key = os.environ[\"HF_API_KEY\"]\n",
    "\n",
    "# Initialize the speaker diarization pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\", use_auth_token=HF_API_Key)\n",
    "\n",
    "# Path to the directory containing the .wav files\n",
    "wav_directory = \"~/wd/GitHub/Research/Ops-Research/\"  # Adjust this path as needed\n",
    "\n",
    "audio_file_path = \"~/wd/GitHub/Research/Ops-Research/audio_files/audio.wav\"\n",
    "#transcription_file = \"~/wd/GitHub/Research/Ops-Research/transcription_files\n",
    "\n",
    "def transcribe_audio_with_speaker_diarization(audio_file_path):\n",
    "    # Perform transcription using OpenAI's Whisper\n",
    "    with open(audio_file_path, 'rb') as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file)\n",
    "    print(transcript.txt)\n",
    "    # Perform speaker diarization using pyannote.audio\n",
    "    diarization = pipeline({'uri': audio_file_path, 'audio': audio_file_path})\n",
    "\n",
    "    # Combine transcription with speaker labels\n",
    "    transcribed_text = transcript['text']   \n",
    "    speaker_segments = []\n",
    "\n",
    "    for segment, track, label in diarization.itertracks(yield_label=True):\n",
    "        speaker_segments.append((segment.start, segment.end, label))   \n",
    "    labeled_transcription = []\n",
    "\n",
    "    for start, end, speaker in speaker_segments:\n",
    "        # Match the speaker segments with the transcription (approximate)\n",
    "        labeled_transcription.append(f\"Speaker {speaker}: {transcribed_text}\")\n",
    "\n",
    "    return \"\\n\".join(labeled_transcription)\n",
    "\n",
    "def process_wav_files(directory):\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".wav\"):\n",
    "            audio_file_path = os.path.join(directory, file_name)\n",
    "            transcription_file = os.path.splitext(file_name + \"_transcription.txt\") \n",
    "    print(transcription_file)\n",
    "\n",
    "# Create a .srt file with the transcribed text\n",
    "def create_srt_file(transcription_file):\n",
    "    with open(transcription_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    srt_file = os.path.splitext(transcription_file)[0] + \".srt\"\n",
    "    with open(srt_file, 'w') as f:\n",
    "        for i, line in enumerate(lines):\n",
    "            f.write(f\"{i+1}\\n\")\n",
    "            f.write(line)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "return srt_file\n",
    "\n",
    "print(f\"SRT file created: {srt_file}\")\n",
    "\n",
    "srt_file_path = \"~/wd/GitHub/Research/Ops-Research/srt_files/audio.srt\"\n",
    "\n",
    "srt_file.save(srt_file_path)\n",
    "\n",
    "print(f\"SRT file saved: {srt_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_wav_files(wav_directory)\n",
    "    create_srt_file(transcription_file)    \n",
    "\n",
    "# This is a complete mess and absolute worthless code that is mainly storing thoughts for the code that will be written in the future.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "standard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
